{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_text(data_path):\n",
    "    text = open(data_path).readlines()\n",
    "    # Convert our text to all lowercase\n",
    "    # text = text.lower()\n",
    "    \n",
    "    #print(\"Our text has {} characters:\".format(len(text)))\n",
    "    # Show first 100 characters\n",
    "    \n",
    "    \n",
    "    text = [\"{} \\n\".format(line[:-1]) for line in text]\n",
    "    text = \" \".join(text)\n",
    "    print(text[:10] + \"...\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are ...\n",
      "I love you...\n"
     ]
    }
   ],
   "source": [
    "# Get the filepath and load the text in\n",
    "training_data_path = './data2/training_data.txt'\n",
    "validation_data_path = './data2/validation_data.txt'\n",
    "\n",
    "training_text = import_text(training_data_path)\n",
    "validation_text = import_text(validation_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text_split = training_text.split(' ')\n",
    "validation_text_split = validation_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1703 unique words in our dataset:\n",
      "['', '\\n', '\"boytoy\"', '#1', '(I', '(PS', '(WOAHHHH)', '(esp', '(lol', '(the', '1', '100%', '11', '18', '18th', '2', '20', '200', '2400', '3.8', '3?', '3rd', '4', '6', '6th', '7', '98', ':)', ':D', ':P', ';)', '<3', '@$$', 'A', 'AND', 'Across', 'After', 'Airbend', 'All', 'Almost', 'Along', 'Although', 'America', 'And', 'Anniversary', 'Another', 'Any', 'Anyone', 'Anything', 'Anyway', 'Anyway,', 'Arabs', 'Are', \"Aren't\", 'Army', 'As', 'At', 'Attend', 'BEAUTIFUL', 'Barbie', 'Bb,', 'Bc', 'Be', 'Because', 'Beijing', 'Believe', 'Better', 'Beyonce', 'Bopster', 'Bourgeoisie', 'Bundled', 'Burns', 'But', 'By', 'CUTE', 'Cause', 'China', 'Christmas', 'Cmere', 'Come', 'Compared', 'Comrade', 'Cum', 'DO', 'Day', 'Day!!!!!!!', 'Death', 'Deep,', \"Didn't\", 'Diegan', 'Do', 'Does', \"Doesn't\", 'Doesnâ€™t', \"Don't\", 'Dw,', 'Earth', 'Either', 'Especially', 'Even', 'Every', 'Everything', 'Except', 'FEAR', 'Fabulous', 'First', 'For', 'Free', 'From', 'G', 'General', 'Get', 'Giving', 'Good', 'Grows', 'Guess', 'HU', \"HU's\", 'HUâ€™s', 'Hancock', 'Happiest', 'Happy', 'Hate', 'Haters', 'Have', 'He', 'Heehee', 'Her', 'Here', \"Here's\", 'Hey', 'His', 'Hollidays!', 'Hope', 'Hopefully', 'How', 'However,', 'Hu', \"Hu's\", 'Hu?', 'I', \"I'd\", \"I'll\", \"I'm\", \"I've\", 'IQ', 'IT', 'Id', 'Idk', 'If', 'Ik', 'In', 'Including', 'Independence', 'Infected', 'Instead', 'Into', 'Is', 'It', \"It'll\", \"It's\", 'Italian', 'Italy', \"Italy's\", 'Its', 'Itâ€™s', 'Iâ€™d', 'Iâ€™ll', 'Iâ€™m', 'Iâ€™ve', 'Japan', 'Jk', 'Jk,', 'Jkkkk', 'Joseph', 'Just', 'K?', 'Ken', 'Kori', 'LA', 'Lee', 'Let', \"Let's\", 'Lets', 'Like', 'Linbopster', 'Linbopter', 'Linboptista', 'Lindsey', 'Lindsey,', 'Lindseys', 'Linz', 'Linz,', 'Linzay,', 'Long', 'Look!', 'Looks', 'Lucky', 'Makes', 'Making', 'Marry', 'Masterdebate', 'Maybe', 'McHu', 'Me', 'Mine', 'More', 'Much', 'My', 'NOT', 'Neutrality', 'Nike,', 'No', 'Nonetheless', 'Nor', 'Not', 'Nothing', 'November', 'Nuff', 'Odyssey', 'Of', 'Off', 'Oh', 'Oh!', 'Ok', 'On', 'Once', 'Only', 'Oops,', 'Or', 'Orbit', 'Ought', 'Pandora,', 'Penelope', 'Perhaps', 'Plastic,', 'Playing', 'Please', 'Plugs', 'Poe', 'Point', 'Polar', 'Pon', 'Pon!', 'Proclamation', 'Question', 'Quite', 'Reading', 'Refused', 'Regardless', 'Reliable,', 'Right', 'Ripened', 'Rome', 'Roosevelts', 'Roses', 'Rubbing', 'SAT', 'SO', 'Saint', 'San', 'Seriously', 'Shall', 'She', \"She'll\", \"She's\", 'Sheâ€™s', 'Since', 'Siren', 'Skyrocketed', 'So', 'Some', 'Someone', 'Sometime', 'Somewhere', 'Sorry', 'Speaking', 'Specifically', 'Star', 'Star,', 'Starbucks', 'Supreme', 'Surprise', 'Sweet', 'Sweetest', 'Swiss', 'Tahiti', 'Taylor', 'Teachers', 'Text', 'Than', 'Thank', 'Thanks', 'That', \"That'll\", \"That's\", 'The', 'Then', \"There's\", 'Therefore', 'Thereâ€™s', 'These', \"They're\", 'Thinking', 'This', 'Tho', 'Though', 'Thrust', 'Til', 'Time', 'To', 'Today', 'Treaty', 'Trust', 'Try', 'Tutors', 'U', 'Unless', 'Until', 'V', \"Valentine's\", 'Versailles', 'Violets', 'WAS', 'Waiting', 'Wales', 'Wanted', 'Wars', 'Was', 'Wash', 'Watch', 'We', \"We'll\", \"We're\", \"We've\", 'Well', 'Well,', 'Weâ€™ll', 'Weâ€™ve', 'What', \"What's\", 'Whatever', 'Whatever.', 'When', 'Whenever', 'Where', 'Whether', 'Which', 'While', 'Who', 'Whose', 'Why', 'Wish', 'With', 'Without', 'Write', 'Xmas', 'Xoxoxoxox', 'Y', 'Yay', 'Yea,', 'Year', 'You', \"You're\", \"You've\", 'You,', 'Your', 'Youre', 'Youâ€™re', 'a', 'about', 'above', 'absolute', 'acceleration', 'aced', 'across', 'actress', 'actually', 'acute', 'add', 'addicted', 'adds', 'adorable', 'adorably', 'adores', 'adult', 'affection', 'after', 'again', 'against', 'ago', 'ahead', \"ain't\", 'aint', 'air', 'airbend', 'airline', 'alive', 'all', 'almighty', 'almost', 'already', 'alright', 'also', 'although', 'always', 'am', 'amazing', 'amazingly', 'ambition', 'amend', 'among', 'an', 'and', 'annexed', 'anti-self', 'antithesis', 'any', 'anyone', 'anything', 'anythings', 'anyway', 'anywhere', 'apple', 'approval', 'are', 'armpits', 'arms', 'around', 'art', 'artist', 'artistic,', 'artsy', 'as', 'asap', 'ask', 'asked', 'astounding', 'at', 'atm', 'atriums', 'attendant', 'attest', 'attractive', 'awake', 'awards', 'away', 'awesome', 'awesomeness', 'back', 'bad', 'balloon', 'barely', 'barf', 'basically', 'bb', 'bb,', 'bc', 'be', 'bean', 'beat', 'beats', 'beautiful', 'became', 'because', 'bed', 'been', 'beer', 'before', 'beginning', 'behind', 'behind)', 'being', 'believe', 'belly', 'belong', 'best', 'bet', 'better', 'between', 'big', 'bills', 'bird', 'birthday', 'bit', 'bite', 'blast', 'blessed', 'blessing', 'blew', 'blind', 'blind,', 'block', 'blockade', 'blocked', 'blood', 'blow', 'blowjob', 'blows', 'blue', 'body', 'bony', 'bop', 'bopster', 'bored', 'bored,', 'both', 'brain', 'braindead', 'branch', 'breadth', 'break', 'breast', 'breathe', 'bride', 'bright', 'bring', 'bro', 'brother', 'buff', 'bunny', 'buns', 'busy', 'but', 'buy', 'by', 'call', 'called', 'calling', 'calm', 'came', 'campus', 'can', \"can't\", 'cant', 'canâ€™t', 'car', 'carb-y', 'cards', 'care', 'cared', 'career', 'caring', 'carry', 'cashiers', 'cataclysm', 'cave', 'center', 'chain', 'change', 'changed', 'changing', 'charming', 'chat', 'cheating', 'cheer', 'chibi', 'childhood', 'chocolate', 'choose', 'classes', 'clay', 'clearly', 'clothing', 'cocaine', 'cock', 'cockroaches', 'cold', 'color', 'colosseum', 'come', 'comfort', 'compares', 'comparison', 'compassion', 'compassionately', 'competition', 'complete', 'compromise', 'confessed', 'confession', 'conflict', 'connection', 'continue', 'conundrum', 'cook', 'cooking', 'cool', 'cope', 'coray', 'cornforth', 'cough', 'could', \"could've\", \"couldn't\", 'couldnâ€™t', 'counts', 'couple', 'covers', 'crafty,', 'cranium', 'crap', 'crash', 'crave', 'craziest', 'crazy', 'creative', 'cried', 'croissant', 'cuddle', 'cuddled', 'cuddling', 'cult', 'cum', 'curves', 'cute', 'cuteness', 'cutest', 'cutie', 'cutie,', 'cypress', 'dance,', 'darling', 'darlng', 'darn', 'day', 'days', 'de', 'dead', 'dear', 'death', 'debate', 'debaters', 'debt', 'decay', 'dee', 'deep', 'deeper', 'defeat', 'define', 'definitely', 'delayed', 'delays,', 'delicious', 'delight', 'demand', 'depression', 'derpy', 'described', 'describes', 'deserve', 'desire', 'deux', 'developed', 'did', \"didn't\", \"didn't,\", 'dies', 'diligence', 'dimension', 'direction', 'disappear', 'disappoint', 'disaster', 'distance', 'distress', 'divine', 'djegf.....', 'do', 'do?', 'does', \"doesn't\", 'doing', \"don't\", 'donation', 'done', 'dongle', 'donut', 'donâ€™t', 'door', 'double', 'doubt', 'dove', 'down', 'dream', 'dreams', 'dress', 'drive', 'drop', 'dropped', 'drought', 'drug', 'duct', 'during', 'each', 'earlier', 'early', 'early)', 'eat', 'economics', 'ego?', 'eh', 'element', 'else', 'em', 'embrace', 'encapsulate', 'enchanting', 'end', 'ended', 'enjoy', 'enjoyed', 'enough', 'enteral', 'enthalpy', 'entice', 'epic', 'escalate', 'escalated', 'escape', 'et', 'eternal', 'eternity', 'even', 'event', 'ever', 'every', 'everyday', 'everyone', 'everything', 'exceed', 'exception', 'excited', 'exciting', 'exclaim', 'excuse', 'expectations', 'express', 'extraterrestrial', 'extremely', 'eyes', 'face', 'facelift', 'faces', 'fact', 'fails', 'falconer', 'fall', 'fame', 'family', 'far', 'farts', 'fat', 'favorite', 'fb', 'fear', 'feather', 'feeding', 'feel', 'feelings', 'feels', 'feet', 'fell', 'female', 'fight', 'finally', 'financial', 'financially', 'find', 'fine', 'fingering', 'fingernails', 'finish', 'fire', 'first', 'fit', 'flakey', 'flavored', 'flawless', 'flesh', 'flex', 'flight', 'flights', 'flow', 'flute!', 'flutters', 'fly', 'followed', 'fondle', 'fondue', 'fool', 'for', 'force', 'forever', 'forget', 'forgot', 'fornicate', 'forward', 'found', 'free', 'friend', 'from', 'frowns', 'fuel', 'fun', 'fundamental', 'funny', 'fyi', 'gal', 'genitalia', 'get', 'getting', 'gf', 'gift', 'gifts', 'girl', 'girlfriend', 'girls', 'give', 'given', 'gives', 'gjejafjcfg', 'glad', 'glands', 'gloomy', 'gnight', 'go', 'god', \"god's\", 'god-sent', 'goddess', 'goes', 'going', 'gone', 'gone?', 'gonna', 'good', 'got', 'gotta', 'governs', 'grabs', 'grace', 'grande', 'grapes', 'great', 'greed', 'greys', 'grin', 'grind', 'groans', 'grow', 'guess', 'guy', 'had', 'hair', 'half', 'hall', 'hand', 'handle', 'hands', 'happen', 'happiest', 'happy', 'hard', 'hardens', 'has', 'hashbrown', 'hate', 'hatter', 'have', \"haven't\", 'having', \"he's\", 'head', 'headache', 'hear', 'heart', 'heaven', 'heavy', 'help', 'her', 'here', \"here's\", 'hereâ€™s', 'heyyyy', 'high', 'hips', 'his', 'home', 'home,', 'hope', 'hope)', 'hopefully', 'hormones', 'host', 'hostility', 'hot', 'hot)', 'hotness', 'hottie', 'hour', 'hours', 'house', 'how', 'hubbie', 'hug', 'hug,', 'hugs', 'hush', 'huuuuuuu???', 'i', 'id', 'ideal', 'idk', 'idol', 'if', 'ignore', 'ignored', 'ik', 'ill', 'im', 'impassion', 'imperfect', 'impress', 'imprisoned', 'in', 'incarnate', 'inches', 'incredibly', 'infinite', 'infinitely', 'inflate', 'inflation', 'inform', 'innate', 'inside', 'inspiration', 'institutions', 'intelligent', 'intended', 'into', 'invented', 'invention', 'inviting', 'involve', 'irate', 'irresistable', 'is', 'is,', 'isnâ€™t', 'isolation', 'it', \"it'll\", \"it's\", 'it?', 'its', 'itâ€™s', 'jabboo', 'jealous,', 'jelly', 'jolly', 'joy', 'just', 'keep', 'keeper', 'keys', 'kidding', 'kind', 'kinda', 'kiss', 'kisses', 'kissessss', 'knew', 'knife', 'knot', 'know', 'know,', 'lacks', 'land', 'last', 'late', 'laughs', 'learn', 'least', 'leave', 'leaves', 'left', 'legs', 'lemme', 'lemon', 'less', 'let', \"let's\", 'life', 'lifeguard', 'lifetime', 'light', 'like', 'liked', 'lil', 'limbs', 'linZ', 'linZ,', 'linbop', 'linbop,', 'linbopster', 'line', 'linz', 'linz,', 'linzay', 'linzays', 'linzz', 'list', 'little', 'live', 'living', 'loneliness', 'lonely', 'long', 'long?', 'longer', 'look', 'looking', 'looks', 'lorikeets', 'lose', 'lost', 'lost,', 'lot', 'lots', 'love', 'love,', 'loveable', 'loved', 'lovely', 'lover', 'loves', 'lovesick', 'loving', 'loyal', 'luck', 'luck,', 'luckiest', 'lucky', 'lunch', 'lust', 'lusting', 'lyrics', 'mad', 'made', 'make', 'make-uped,', 'makes', 'mall', 'man', 'many', 'marble', 'marry', 'masochism', 'mass', 'massaging', 'master', 'matches', 'mate', 'math', 'matter', 'mattress', 'mav', 'may', 'maybe', 'maybeee', 'me', 'me,', 'me?', 'meals', 'mean', 'mean,', 'means', 'meant', 'meatballs', 'mein', 'mention', 'merry', 'mess', 'messed', 'met', 'metaphorical', 'mice', 'mile', 'mind', 'mine', 'miracle', 'misconception', 'miss', 'missed', 'moment', 'more', 'more...poetic', 'moreâ€', 'morning', 'most', 'most,', 'mothers', 'motivate', 'move', 'moves', 'movie', 'mrs.', 'msg', 'much', 'much,', 'mucho', 'mud', 'murder', 'must', \"must've\", 'mute', 'my', 'myself', 'nails', 'naivete', 'naked', 'name', 'nasty', 'nation', 'navigation', 'nearly', 'necessity', 'need', 'need?', 'needs,', 'neglecting', 'never', 'new', 'next', 'nice', 'night', 'nights', 'nightâ€™s', 'no', 'nominating', 'nonetheless', 'noon', 'nose', 'not', 'nothing', 'now', 'nude', 'number', 'object', 'obsessed', 'ocean', 'of', 'off', 'oh', 'ok', 'ok,', 'old', 'olive', 'on', 'on,', 'onboard', 'once', 'one', \"one's\", 'ones', 'only', 'onto', 'onward', 'opening', 'or', 'order', 'other', 'our', 'out', 'over', 'overall', 'overreacting', 'overwhelming', 'package', 'page', 'pain', 'painful', 'paint', 'paints', 'pair', 'pale', 'pales', 'paradise', 'park', 'part', 'parties', 'partnered', 'pas', 'passes', 'passion', 'past', 'patient', 'pay', 'pedophile', 'peg', 'penis', 'perfect', 'perfect,', 'perfection', 'perfectly', 'period', 'perks', 'perm', 'permit', 'persistent,', 'person', 'perverted', 'petition', 'philosopher', 'photos', 'pic', 'pilot', 'piss-poor', 'place', 'plane', 'planned', 'please', 'pleasure', 'poem', 'poem!', 'poem?', 'poems', 'poems?', 'poet', 'poetry', 'point', 'poor', 'posters,', 'pound', 'pour', 'powerful', 'precious', 'prettiest', 'pretty', 'pride', 'princess', 'priority', 'prob', 'probably', 'problem', 'procreate', 'procreation', 'professed', 'project', 'promise', 'provides', 'psyched', 'published', 'puddle', 'pull', 'pulling', 'pure', 'push', 'put', 'question', 'quite', 'radiance', 'rain', 'rainbow', 'ran', 'ransacked', 'rate', 'rather', 'reacting', 'read', 'realize', 'really', 'rear', 'reasonable', 'reasons', 'reception', 'receptions', 'reciprocals', 'recover', 'red', 'refer', 'references', 'reflection', 'regards', 'remind', 'repetition', 'request', 'requires', 'respond', 'rest', 'restaurant', 'revolution', 'rhapsodies', 'rhyme', 'rhymes', 'rich', 'ride', 'right', 'ring', 'rises', 'rly', 'robot', 'robots', 'rod', 'rode', 'roof', 'roommates', 'root', 'roses', 'rough', 'round', 'rounds', 'rowdy', 'rush', 'sad', 'sad..', 'sadistic', 'safari', 'said', 'sakes', 'same', 'sate', 'satiate', 'sausage', 'save', 'saving', 'saw', 'say', 'say...', 'saying', 'says', 'scenario', 'sea', 'second', 'see', 'seem', 'seen', 'self', 'send', 'sensation', 'sense', 'sent', 'separated', 'set', 'setting', 'settle', 'sever', 'sexiest', 'sext', 'sexy', 'shade', 'shaft', 'shake', 'shall', 'sharp', 'shaves', 'she', \"she'll\", \"she's\", 'sheâ€™s', 'shine', 'shines', 'shining', 'shiny', 'shoes', 'shoot', 'shopping', 'short', 'should', 'shouldâ€™ve', 'show', 'shrew', 'sick', 'sick)', 'signed', 'silly', 'since', 'sing', 'single', 'sitting', 'sketches', 'skilled', 'skills', 'sky?', 'sleep', 'sleepy', 'slides', 'slightly', 'slowly', 'small', 'smart', 'smartest', 'smartest,', 'smartie,', 'smell', 'smile', 'smiles', 'smooth-skin', 'snap', 'snapchat', 'snore', 'snow', 'so', 'soft', 'softly', 'some', 'someone', 'something', 'sometime', 'somewhere', 'song', 'soo', 'soon', 'sooo', 'soooo', 'sorrow', 'sorry', 'sorry,', 'soso', 'soulmate', 'sour', 'soâ€¦', 'space', 'spaghetti', 'sparkles', 'sparkling', 'speaker', 'special', 'speechless', 'speed', 'spend', 'spending', 'spoiler:', 'spreeeee', 'squirms', 'stake', 'start', 'starting', 'starts', 'state', 'station', 'statues', 'stay', 'steaks', 'stick', 'still', 'stop', 'stops', 'store', 'story', 'strange', 'strike', 'stuff', 'stunningly', 'stunningness', 'substitute', 'succeed', 'such', 'suck', 'sucks', 'suitors', 'summer', 'sun', 'super', 'superb', 'support', 'sure', 'surprise', 'surrounds', 'swear', 'sweat', 'sweep', 'sweet', 'sweetest', 'sweetheart', 'swept', 'swift', 'swim', 'symbolic', 'take', 'taking', 'talent', 'talented', 'talented,', 'talk', 'talking', 'tall', 'tape', 'task', 'tasked', 'tasty', 'tea', 'team', 'tell', 'temporary', 'tender', 'tests', 'text', 'than', 'thanks', 'that', \"that's\", 'the', 'them', 'then', 'then,', 'there', \"there's\", 'there,', 'these', 'theyâ€™re', 'thigh', 'thing', 'things', 'think', 'thinking', 'this', 'those', 'though', 'though,', 'thoughtfulness', 'thoughts', 'throb', 'through', 'thrust', 'thts', 'tim', 'time', 'times', 'tingle', 'tired', 'tmr', 'to', 'today', 'together', 'told', 'tomorrow', 'ton', 'tonight', 'too', 'too!', 'took', 'top', 'total', 'totally', 'touch', 'touched', 'tough', 'trade', 'transcend', 'transcendent', 'trash', 'treats', 'tree', 'tribute', 'tricks', 'tried', 'trinkets', 'true', 'truffle', 'truffles', 'truly', 'trust', 'try', 'trying', 'tuck', 'turn', 'turned', 'turns', 'tutor', 'twice', 'two', 'uncalled', 'uncontrollable', 'under', 'understand', 'ungraceful,', 'until', 'unwell', 'up', 'uplifts', 'upset', 'upset,', 'us', 'useful', 'utc?', 'vary', 'ventricles', 'verify', 'very', 'violet', 'vision', 'visit', 'vj', 'voice', 'vow', 'wait', 'wake', 'wakes', 'walk', 'wanna', 'want', 'wanted', 'wants', 'warm', 'warn', 'warning', 'was', \"wasn't\", 'way', 'way,', 'ways', 'we', \"we'll\", \"we're\", 'weary', 'wedding', 'weed', 'week', 'weekend', 'weeks', 'weigh', 'well', 'well,', 'went', 'were', \"weren't\", 'wetter', 'weâ€™ll', 'weâ€™re', 'what', 'when', 'whenever', 'where', 'wherever', 'whether', 'which', 'while', 'whites', 'who', 'whole', 'whole,', 'whom', 'why', 'wide', 'wife', 'wife.', 'will', 'win', 'wins', 'wish', 'wishing', 'with', 'within', 'without', 'woke', 'women', 'won', 'wonder', 'wonderful', 'wonâ€™t', 'woo', 'words', 'work', 'world', 'worry', 'worth', 'would', 'wouldnâ€™t', 'wow', 'write', 'writing', 'wrong', 'wrote', 'wth', 'wunderful', 'yay,', 'year', 'yearning', 'years', 'yes', 'you', 'you!', 'you!!', \"you'd\", \"you'll\", \"you're\", \"you's\", \"you've\", 'you,', 'you...', 'you?', 'your', 'yours', 'youuuuu', 'youâ€™re', 'zero', 'â€œHEYYYYYYâ€', 'â€œI', 'â¤ï¸â¤ï¸â¤ï¸â¤ï¸', 'â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸', 'ðŸ˜˜ðŸ˜˜ðŸ˜˜ðŸ˜˜ðŸ˜˜ðŸ˜˜ðŸ˜˜ðŸ˜˜']\n"
     ]
    }
   ],
   "source": [
    "# Get all unique characters from our file\n",
    "vocab = sorted(set(training_text_split + validation_text_split))\n",
    "print('We have {} unique words in our dataset:'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary between each unique characters and an \n",
    "# index (e.g. 'a' maps to 1, 'b' maps to 2, etc)\n",
    "word2idx = {word:index for index, word in enumerate(vocab)}\n",
    "idx2word = np.array(vocab)\n",
    "\n",
    "training_text_as_idx = np.array([word2idx[w] for w in training_text_split])\n",
    "validation_text_as_idx = np.array([word2idx[w] for w in validation_text_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence (number of characters) we want \n",
    "# for a single input of data in our model.\n",
    "# The bigger it is, the more evidence our model has\n",
    "seq_length = 16\n",
    "\n",
    "# Number of batches before we finish 1 epoch (training on all data once)\n",
    "examples_per_epoch = len(training_text) // seq_length\n",
    "\n",
    "print(examples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each input is 128 characters (as defined in seq_length). But we can train in batches of multiple inputs. \n",
    "# BATCH_SIZE = how many inputs to train on at once\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning models predict things by training on data.\n",
    "# In our case, given a bunch of characters, we try to predict the \n",
    "# next letter to match our training data.\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(text_as_idx):\n",
    "    # Make a tensorflow dataset\n",
    "    word_dataset = tf.data.Dataset.from_tensor_slices(text_as_idx)\n",
    "\n",
    "    # Split the data into batches\n",
    "    sequences = word_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "    for item in sequences.take(1):\n",
    "        print(repr(' '.join(idx2word[item.numpy()])))\n",
    "\n",
    "\n",
    "    # We split the dataset into evidence (characters we know) and \n",
    "    # targets (the next character to predict) using the function defined above\n",
    "    dataset = sequences.map(split_input_target)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Roses are red \\n Violets are blue \\n I hope you love me \\n Cause I love'\n",
      "\"I love you like no other \\n And even if you're not here to see \\n Across\"\n"
     ]
    }
   ],
   "source": [
    "training_dataset = gen_dataset(training_text_as_idx)\n",
    "validation_dataset = gen_dataset(validation_text_as_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 1024\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 2048\n",
    "\n",
    "DROPOUT_PROB=.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about it. Basically, if we're using a graphics card, we can apply special optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            recurrent_initializer='glorot_uniform',\n",
    "                            activation='relu',\n",
    "                            stateful=True, \n",
    "                            recurrent_activation='hard_sigmoid',\n",
    "                            use_bias=True,\n",
    "                            dropout=DROPOUT_PROB,\n",
    "                            recurrent_dropout=DROPOUT_PROB\n",
    "                           ),\n",
    "        tf.keras.layers.Dense(128, activation='relu', use_bias=True),\n",
    "        tf.keras.layers.Dropout(DROPOUT_PROB),\n",
    "        tf.keras.layers.Dense(vocab_size, use_bias=True)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4031: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units,\n",
    "    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16, 1703) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in validation_dataset.take(1): \n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (16, None, 1024)          1743872   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (16, None, 2048)          18880512  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (16, None, 128)           262272    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (16, None, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (16, None, 1703)          219687    \n",
      "=================================================================\n",
      "Total params: 21,106,343\n",
      "Trainable params: 21,106,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(16), Dimension(16)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=20,\n",
    "                                                   batch_size=BATCH_SIZE, write_graph=True, \n",
    "                                                   write_grads=True, write_images=False, \n",
    "                                                   embeddings_freq=20, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:1730: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "Epoch 1/30\n",
      "157/157==============================] - 423s 3s/step - loss: 140.1371 - val_loss: 7.3307\n",
      "Epoch 2/30\n",
      "157/157==============================] - 467s 3s/step - loss: 7.2615 - val_loss: 7.2049\n",
      "Epoch 3/30\n",
      "157/157==============================] - 445s 3s/step - loss: 7.1328 - val_loss: 7.0812\n",
      "Epoch 4/30\n",
      " 81/157==============>...............] - ETA: 3:21 - loss: 7.0362"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "history = model.fit(training_dataset.repeat(),\n",
    "                    epochs=EPOCHS,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "                    validation_data=validation_dataset.repeat(),\n",
    "                    validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # You can change the start string to experiment\n",
    "    start_string = 'roses'\n",
    "  \n",
    "    # Converting our start string to numbers (vectorizing) \n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "  \n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "  \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "  \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "  \n",
    "        # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "  \n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(model, start_string=\"roses\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
