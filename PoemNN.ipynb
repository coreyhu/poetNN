{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_text(data_path):\n",
    "    text = open(data_path).readlines()\n",
    "    # Convert our text to all lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #print(\"Our text has {} characters:\".format(len(text)))\n",
    "    # Show first 100 characters\n",
    "    \n",
    "    \n",
    "    text = [\"{} \\n\".format(line[:-1]) for line in text]\n",
    "    text = \" \".join(text)\n",
    "    print(text[:10] + \"...\")\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roses are ...\n",
      "I love you...\n"
     ]
    }
   ],
   "source": [
    "# Get the filepath and load the text in\n",
    "training_data_path = './data2/training_data.txt'\n",
    "validation_data_path = './data2/validation_data.txt'\n",
    "\n",
    "training_text = import_text(training_data_path)\n",
    "validation_text = import_text(validation_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text_split = training_text.split(' ')\n",
    "validation_text_split = validation_text.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream"
    }
   ],
   "source": [
    "# Get all unique characters from our file\n",
    "vocab = sorted(set(training_text_split + validation_text_split))\n",
    "print('We have {} unique words in our dataset:'.format(len(vocab)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dictionary between each unique characters and an \n",
    "# index (e.g. 'a' maps to 1, 'b' maps to 2, etc)\n",
    "word2idx = {word:index for index, word in enumerate(vocab)}\n",
    "idx2word = np.array(vocab)\n",
    "\n",
    "training_text_as_idx = np.array([word2idx[w] for w in training_text_split])\n",
    "validation_text_as_idx = np.array([word2idx[w] for w in validation_text_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2518\n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence (number of characters) we want \n",
    "# for a single input of data in our model.\n",
    "# The bigger it is, the more evidence our model has\n",
    "seq_length = 16\n",
    "\n",
    "# Number of batches before we finish 1 epoch (training on all data once)\n",
    "examples_per_epoch = len(training_text) // seq_length\n",
    "\n",
    "print(examples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each input is 128 characters (as defined in seq_length). But we can train in batches of multiple inputs. \n",
    "# BATCH_SIZE = how many inputs to train on at once\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "steps_per_epoch = examples_per_epoch // BATCH_SIZE\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine learning models predict things by training on data.\n",
    "# In our case, given a bunch of characters, we try to predict the \n",
    "# next letter to match our training data.\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_dataset(text_as_idx):\n",
    "    # Make a tensorflow dataset\n",
    "    word_dataset = tf.data.Dataset.from_tensor_slices(text_as_idx)\n",
    "\n",
    "    # Split the data into batches\n",
    "    sequences = word_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "    for item in sequences.take(1):\n",
    "        print(repr(' '.join(idx2word[item.numpy()])))\n",
    "\n",
    "\n",
    "    # We split the dataset into evidence (characters we know) and \n",
    "    # targets (the next character to predict) using the function defined above\n",
    "    dataset = sequences.map(split_input_target)\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "'Roses are red \\n Violets are blue \\n I hope you love me \\n Cause I love'\n",
      "\"I love you like no other \\n And even if you're not here to see \\n Across\"\n"
     ]
    }
   ],
   "source": [
    "training_dataset = gen_dataset(training_text_as_idx)\n",
    "validation_dataset = gen_dataset(validation_text_as_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 1024\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 2048\n",
    "\n",
    "DROPOUT_PROB=.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't worry about it. Basically, if we're using a graphics card, we can apply special optimizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.GRU(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            recurrent_initializer='glorot_uniform',\n",
    "                            activation='relu',\n",
    "                            stateful=True, \n",
    "                            recurrent_activation='hard_sigmoid',\n",
    "                            use_bias=True,\n",
    "                            dropout=DROPOUT_PROB,\n",
    "                            recurrent_dropout=DROPOUT_PROB\n",
    "                           ),\n",
    "        tf.keras.layers.Dense(128, activation='relu', use_bias=True),\n",
    "        tf.keras.layers.Dropout(DROPOUT_PROB),\n",
    "        tf.keras.layers.Dense(vocab_size, use_bias=True)\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tensorflow/python/keras/backend.py:4031: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    vocab_size = len(vocab),\n",
    "    embedding_dim = embedding_dim,\n",
    "    rnn_units = rnn_units,\n",
    "    batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16, 1703) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in validation_dataset.take(1): \n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (16, None, 1024)          1743872   \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (16, None, 2048)          18880512  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (16, None, 128)           262272    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (16, None, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (16, None, 1703)          219687    \n",
      "=================================================================\n",
      "Total params: 21,106,343\n",
      "Trainable params: 21,106,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(16), Dimension(16)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_loss.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.train.AdamOptimizer(),\n",
    "    loss = loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Weight and gradient histograms not supported for eagerexecution, setting `histogram_freq` to `0`.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=20,\n",
    "                                                   batch_size=BATCH_SIZE, write_graph=True, \n",
    "                                                   write_grads=True, write_images=False, \n",
    "                                                   embeddings_freq=20, update_freq='batch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "history = model.fit(training_dataset.repeat(),\n",
    "                    epochs=EPOCHS,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    callbacks=[checkpoint_callback, tensorboard_callback],\n",
    "                    validation_data=validation_dataset.repeat(),\n",
    "                    validation_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Poems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 1024)           1743872   \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 2048)           18880512  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (1, None, 128)            262272    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (1, None, 128)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, None, 1703)           219687    \n",
      "=================================================================\n",
      "Total params: 21,106,343\n",
      "Trainable params: 21,106,343\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # You can change the start string to experiment\n",
    "    start_string = 'roses'\n",
    "  \n",
    "    # Converting our start string to numbers (vectorizing) \n",
    "    input_eval = [word2idx[s] for s in start_string.split()]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "  \n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "  \n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "  \n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "  \n",
    "        # using a multinomial distribution to predict the word returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.multinomial(predictions, num_samples=1)[-1,0].numpy()\n",
    "        \n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "        \n",
    "        text_generated.append(idx2word[predicted_id])\n",
    "  \n",
    "    return (start_string + ' '.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roseswhether first asap 4 soulmate Yea, With complete carry with imprisoned poem pay Oops, escape Thanks there amazing look Pandora, there calling Until mud will best cheer say whenever Tahiti Didn't girl linbopster And his Another list visit All if  first element wait feet compassionately almighty called developed you've me and bird bean be space barely math down And could've let's Fabulous morning from I’ll shaves bit always He What's let loved rhymes trinkets  perfect all uplifts Ought cried hate But needs, tell shoes financially demand When exciting tricks So tell FEAR poem person WAS  center lil On it about thoughtfulness together addicted lovesick sorry, park Poe mate early) two (the You’re kiss beer linzz I’m atm nude friend you're I'm you're grow Get comparison  inform mrs. fact with thing Neutrality awesome about nice much only November for wanted You're compassionately Compared  knew day Even Ought cutie, complete smart give so means you’re yours grapes sketches sense :) shrew @$$ me wedding lot cum as snapchat Bc Or Proclamation went I'm Bc for Hope naivete nose smartie, Bc imprisoned matches confession They're for great can't more...poetic only pull disaster make-uped, there's The so provides maybe shine found provides with lunch couple irresistable during hopefully donation choose not acceleration lemme No pale gifts stop voice it need? with way to I'll surrounds actress mine touch me little Marry bride love mud atriums inside should’ve state Polar long? the Roses Until soso transcendent The Violets overall make Arabs which out \n",
      " marble flakey more don't Quite promise well the Lindsey pulling hope red hope in Hu your There's soulmate add long After your thrust world be inviting adds ever chocolate into excuse since powerful lil calling now \n",
      " wow not think we’ll Id naivete Be moves say I poor NOT task shall cockroaches most, snore Siren perfectly Tahiti truly you poems smile done skilled encapsulate shopping wide fat Violets across gloomy you braindead Write as free beer Roses with greys still Cause setting like repetition breathe I loneliness mine text with We’ll have night me, acute Polar woke perverted  learn ok CUTE those don't cold dance, took perfection silly is, short with And economics it’s need LA I'd roses with he's asap In long? From Jk, skills im loved What His way god feelings pure blue everyday basically words back It's no ocean ain't soso olive off  flakey \n",
      " linz 200 acute radiance poem as took hush fire happen hugs lonely that you's goes never You're sick can’t like when set penis wonder heaven Penelope exciting cared only get <3 Especially For blow Rome rhapsodies while turned about paradise expectations always Violets She'll actually make ;) \n",
      " And now acceleration overall debaters tough while big morning look sensation talented blue every up bb you way Don't my And provides Here back poor Guess long Along breathe wish Pon! find snow alive such transcend Oops, been truly you... cutie, while faces starts help red divine uncalled love who me? For without sweetest don't dimension tall While nonetheless heart you're too skills disappoint 18th whole feel much Including double alive grande HU poem rhymes heart professed better lifetime repetition ones Because I've before a among Whether Oops, a pic I’d we’ll ocean Bc lunch needs, fondue impassion Right Xoxoxoxox really first cant up first procreate someone Somewhere already two shopping was paint Violets Please its ever wife. together posters, myself derpy sent Linz, :D you then, chibi sure Nor calling (PS Are you too by I’ve anyone 3rd acute By his loyal Another forgot getting you lose thts met of derpy First Christmas <3 verify the more fine were moment weed artsy can't everything blessing But feel cute my called belong scenario first no miss think You Nuff can't olive wife me person Anyone perfectly When turn Oh distress page let's forever call cornforth \n",
      " called world thoughtfulness necessity any  much steaks sooo as fame busy linzays poor Tahiti you're rather be I Tho For want astounding love nation Jkkkk vj out fornicate worth as seen good will me you, week its HU's uplifts the of 18 3? many starts reacting Violets amazing else gone skills heaven aced encapsulate drop like blessed id while genitalia rhapsodies team to take put guess somewhere Its San blue last What only question satiate mattress Like Didn't lorikeets artsy to old  fingernails work chain Bc Time land I'm somewhere wonderful describes man truffle my Much be make Bb, look meant Playing pale peg mean wanna clothing happy Linzay, push by blind Good there's much overall can’t forever tea The Lindsey loving thigh so bunny And away my morning sorry gotta after too restaurant Your throb disaster inflate inside pain darlng Then So early rest compassionately Long linbop prettiest story You've Wish V being Write why tim And Except point I second Wars for Maybe came Roses to face for lusting change career plane wth and SO ransacked mine huuuuuuu??? least My duct impress expectations anyone time remind We've you arms pedophile smart poem for the saving pound on liked HU's soso 18th long compromise makes belong while feel  where suitors pedophile forget sketches within treats means <3 If forget Tutors really Almost you’re around fun the Guess you're perfect keep calm had someone happen scenario talk Xmas turned recover airbend earlier ungraceful, There's that you, \n",
      " away buns escalated right at stop you my You're Therefore this mean describes lorikeets Versailles hours forget fact gloomy genitalia me lacks saving on, groans smart Army in Bc We're nails haven't things in Compared bored, told described you, chat point Bc sorry, smiles up least posters, do Plastic, uncalled So Question do SO So Lindsey remind bet you're depression Its color America trinkets blows found From feelings hostility that's passion text light smart come Day!!!!!!! So linbopster there its beats BEAUTIFUL truffle lifetime happy financially gifts sleepy send all is, groans penis sent warm hair glad beautiful had turns walk that's\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=\"roses are red, violets are blue\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
